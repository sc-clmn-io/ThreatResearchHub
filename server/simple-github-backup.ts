import fs from 'fs';
import path from 'path';

interface GitHubBackupConfig {
  token: string;
  username: string;
  repository: string;
  branch: string;
  commitMessage: string;
}

interface BackupResult {
  success: boolean;
  message: string;
  timestamp: string;
  files?: string[];
  contentLibrary?: boolean;
  deploymentReady?: boolean;
}

export async function simpleGitHubBackup(config: GitHubBackupConfig): Promise<BackupResult> {
  const { token, username, repository, branch, commitMessage } = config;
  const timestamp = new Date().toISOString();

  try {
    // Get all project files to backup
    const files = await collectProjectFiles();
    
    // Create backup content
    const backupContent = await createBackupContent(files);
    
    // Upload to GitHub using GitHub API instead of git commands
    const result = await uploadToGitHub(config, backupContent);
    
    return {
      success: true,
      message: `XSIAM content library backup complete - ${files.length} files ready for production deployment`,
      timestamp,
      files,
      contentLibrary: true,
      deploymentReady: true
    };
    
  } catch (error) {
    console.error('Backup error:', error);
    return {
      success: false,
      message: `Backup failed: ${error instanceof Error ? error.message : 'Unknown error'}`,
      timestamp
    };
  }
}

async function collectProjectFiles(): Promise<string[]> {
  const files: string[] = [];
  const includePatterns = [
    // Complete XSIAM Content Directory
    'content',
    
    // Infrastructure Automation
    'infra',
    
    // Core Platform Files
    'client/src',
    'server',
    'shared',
    
    // Configuration Files
    'package.json',
    'tsconfig.json',
    'vite.config.ts',
    'tailwind.config.ts',
    'drizzle.config.ts',
    'README.md',
    'replit.md',
    
    // Documentation
    'GITHUB_EXPORT_GUIDE.md',
    'DEVELOPMENT_MANIFEST.md',
    'CONTRIBUTING.md',
    'docs'
  ];
  
  const excludePatterns = [
    '.git',
    'node_modules',
    'dist',
    '.replit',
    'attached_assets',
    'backups',
    '.DS_Store',
    'package-lock.json'
  ];
  
  function scanDirectory(dir: string, relativePath = '') {
    if (!fs.existsSync(dir)) return;
    
    const items = fs.readdirSync(dir);
    
    for (const item of items) {
      const fullPath = path.join(dir, item);
      const relPath = path.join(relativePath, item);
      
      // Skip excluded patterns
      if (excludePatterns.some(pattern => relPath.includes(pattern))) {
        continue;
      }
      
      const stat = fs.statSync(fullPath);
      
      if (stat.isDirectory()) {
        scanDirectory(fullPath, relPath);
      } else if (stat.isFile()) {
        files.push(relPath);
      }
    }
  }
  
  // Scan specific directories for XSIAM content
  for (const pattern of includePatterns) {
    const fullPath = path.join(process.cwd(), pattern);
    if (fs.existsSync(fullPath)) {
      if (fs.statSync(fullPath).isDirectory()) {
        scanDirectory(fullPath, pattern);
      } else {
        files.push(pattern);
      }
    }
  }
  
  // Ensure content directory is fully included
  ensureContentDirectorySync(files);
  
  console.log(`üìã Collected ${files.length} files for backup including complete content/ directory`);
  
  // Debug: Show content files being backed up
  const contentFiles = files.filter(f => f.startsWith('content/'));
  console.log(`üóÇÔ∏è Content files in backup: ${contentFiles.length}`);
  contentFiles.forEach(f => console.log(`   üìÑ ${f}`));
  
  // Verify all content files are included
  try {
    if (fs.existsSync(path.join(process.cwd(), 'content'))) {
      const actualContentFiles = getContentFilesRecursive('content');
      console.log(`üîç Actual content files on disk: ${actualContentFiles.length}`);
      actualContentFiles.forEach((f: string) => console.log(`   üóÉÔ∏è ${f}`));
      
      const missingFiles = actualContentFiles.filter((f: string) => !files.includes(f));
      if (missingFiles.length > 0) {
        console.log(`‚ö†Ô∏è Missing content files: ${missingFiles.length}`);
        missingFiles.forEach((f: string) => {
          console.log(`   ‚ùå ${f}`);
          files.push(f); // Add missing files
        });
      }
    }
  } catch (error) {
    console.warn('Error verifying content files:', error);
  }
  
  return files;
}

function getContentFilesRecursive(dir: string, basePath = 'content', files: string[] = []): string[] {
  const items = fs.readdirSync(`${process.cwd()}/${dir}`);
  for (const item of items) {
    const fullPath = path.join(process.cwd(), dir, item);
    const relativePath = path.join(basePath, item);
    if (fs.statSync(fullPath).isDirectory()) {
      getContentFilesRecursive(path.join(dir, item), relativePath, files);
    } else {
      files.push(relativePath);
    }
  }
  return files;
}

function ensureContentDirectorySync(files: string[]) {
  // Ensure all content files are included
  const contentDir = path.join(process.cwd(), 'content');
  if (!fs.existsSync(contentDir)) return;
  
  const contentFiles = [];
  
  function scanContentDir(dir: string, relativePath = 'content') {
    const items = fs.readdirSync(dir);
    
    for (const item of items) {
      const fullPath = path.join(dir, item);
      const relPath = path.join(relativePath, item);
      
      const stat = fs.statSync(fullPath);
      
      if (stat.isDirectory()) {
        scanContentDir(fullPath, relPath);
      } else if (stat.isFile()) {
        contentFiles.push(relPath);
        // Add if not already in files array
        if (!files.includes(relPath)) {
          files.push(relPath);
        }
      }
    }
  }
  
  scanContentDir(contentDir);
  console.log(`‚úÖ Content directory sync: ${contentFiles.length} content files ensured in backup`);
}

async function createBackupContent(files: string[]): Promise<Record<string, string>> {
  const content: Record<string, string> = {};
  
  for (const file of files) {
    try {
      const fullPath = path.join(process.cwd(), file);
      const fileContent = fs.readFileSync(fullPath, 'utf8');
      content[file] = fileContent;
    } catch (error) {
      console.warn(`Skipping file ${file}:`, error);
    }
  }
  
  return content;
}

async function uploadToGitHub(config: GitHubBackupConfig, content: Record<string, string>): Promise<void> {
  const { token, username, repository, commitMessage } = config;
  
  // First, save backup locally
  const manifest = {
    timestamp: new Date().toISOString(),
    message: commitMessage,
    files: Object.keys(content),
    fileCount: Object.keys(content).length,
    platform: 'ThreatResearchHub',
    version: '1.0'
  };
  
  const backupDir = path.join(process.cwd(), 'backups', 'github');
  const backupFile = path.join(backupDir, `backup-${Date.now()}.json`);
  
  if (!fs.existsSync(backupDir)) {
    fs.mkdirSync(backupDir, { recursive: true });
  }
  
  fs.writeFileSync(backupFile, JSON.stringify({ manifest, content }, null, 2));
  console.log(`‚úÖ Backup created: ${backupFile}`);
  
  // Now push to actual GitHub repository using GitHub API
  try {
    console.log(`üöÄ Pushing to GitHub: ${username}/${repository}`);
    
    // Get the repository's default branch
    const repoResponse = await fetch(`https://api.github.com/repos/${username}/${repository}`, {
      headers: {
        'Authorization': `token ${token}`,
        'Accept': 'application/vnd.github.v3+json'
      }
    });
    
    if (!repoResponse.ok) {
      throw new Error(`Repository access failed: ${repoResponse.status}`);
    }
    
    const repoData = await repoResponse.json();
    const defaultBranch = repoData.default_branch || 'main';
    
    // Get the current commit SHA from the default branch
    const branchResponse = await fetch(`https://api.github.com/repos/${username}/${repository}/git/refs/heads/${defaultBranch}`, {
      headers: {
        'Authorization': `token ${token}`,
        'Accept': 'application/vnd.github.v3+json'
      }
    });
    
    let parentSha = null;
    if (branchResponse.ok) {
      const branchData = await branchResponse.json();
      parentSha = branchData.object.sha;
    }
    
    // Create blobs for each file
    const fileBlobs: { path: string; sha: string; mode: string }[] = [];
    
    for (const [filePath, fileContent] of Object.entries(content)) {
      const blobResponse = await fetch(`https://api.github.com/repos/${username}/${repository}/git/blobs`, {
        method: 'POST',
        headers: {
          'Authorization': `token ${token}`,
          'Accept': 'application/vnd.github.v3+json',
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          content: Buffer.from(fileContent).toString('base64'),
          encoding: 'base64'
        })
      });
      
      if (!blobResponse.ok) {
        const errorText = await blobResponse.text();
        console.warn(`Failed to create blob for ${filePath}: ${blobResponse.status} - ${errorText}`);
        continue;
      }
      
      const blobData = await blobResponse.json();
      fileBlobs.push({
        path: filePath,
        sha: blobData.sha,
        mode: '100644'
      });
    }
    
    console.log(`üì¶ Created ${fileBlobs.length} file blobs`);
    
    // Debug: Show which content files were successfully processed
    const contentBlobs = fileBlobs.filter(blob => blob.path.startsWith('content/'));
    console.log(`üóÇÔ∏è Content blobs created: ${contentBlobs.length}`);
    contentBlobs.forEach(blob => console.log(`   ‚úÖ ${blob.path}`));
    
    // Verify all expected content directories are represented
    const expectedContentDirs = ['dashboards', 'layouts', 'playbooks', 'use-cases', 'xql-rules'];
    const actualContentDirs = Array.from(new Set(contentBlobs.map(blob => blob.path.split('/')[1])));
    const missingDirs = expectedContentDirs.filter(dir => !actualContentDirs.includes(dir));
    
    if (missingDirs.length > 0) {
      console.log(`‚ö†Ô∏è Warning: Missing content directories in backup: ${missingDirs.join(', ')}`);
    } else {
      console.log(`‚úÖ All expected content directories present: ${actualContentDirs.join(', ')}`);
    }
    
    // Create a tree with all files - FORCE directory structure recreation
    const treeResponse = await fetch(`https://api.github.com/repos/${username}/${repository}/git/trees`, {
      method: 'POST',
      headers: {
        'Authorization': `token ${token}`,
        'Accept': 'application/vnd.github.v3+json',
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        tree: fileBlobs.map(blob => ({
          path: blob.path,
          mode: blob.mode,
          type: 'blob',
          sha: blob.sha
        })),
        // Force complete tree rebuild - do not use base_tree to ensure fresh structure
        // This ensures all directories and files are properly recreated
      })
    });
    
    if (!treeResponse.ok) {
      throw new Error(`Failed to create tree: ${treeResponse.status}`);
    }
    
    const treeData = await treeResponse.json();
    console.log(`üå≥ Created tree: ${treeData.sha}`);
    
    // Create a commit
    const commitData: any = {
      message: commitMessage,
      tree: treeData.sha,
      author: {
        name: 'ThreatResearchHub',
        email: 'security-research-platform@replit.com'
      }
    };
    
    if (parentSha) {
      commitData.parents = [parentSha];
    }
    
    const commitResponse = await fetch(`https://api.github.com/repos/${username}/${repository}/git/commits`, {
      method: 'POST',
      headers: {
        'Authorization': `token ${token}`,
        'Accept': 'application/vnd.github.v3+json',
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(commitData)
    });
    
    if (!commitResponse.ok) {
      throw new Error(`Failed to create commit: ${commitResponse.status}`);
    }
    
    const commitResult = await commitResponse.json();
    console.log(`üìù Created commit: ${commitResult.sha}`);
    
    // Update the branch reference
    const updateRefResponse = await fetch(`https://api.github.com/repos/${username}/${repository}/git/refs/heads/${defaultBranch}`, {
      method: 'PATCH',
      headers: {
        'Authorization': `token ${token}`,
        'Accept': 'application/vnd.github.v3+json',
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        sha: commitResult.sha
      })
    });
    
    if (!updateRefResponse.ok) {
      throw new Error(`Failed to update branch: ${updateRefResponse.status}`);
    }
    
    console.log(`üéâ Successfully pushed to GitHub: https://github.com/${username}/${repository}/commit/${commitResult.sha}`);
    
  } catch (error) {
    console.error('GitHub push failed:', error);
    throw new Error(`Failed to push to GitHub: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
  
  // Also create a quick-access latest backup
  const latestBackupFile = path.join(backupDir, 'latest-backup.json');
  fs.writeFileSync(latestBackupFile, JSON.stringify({
    manifest,
    content
  }, null, 2));
  
  // Create XSIAM content library structure
  const contentLibraryDir = path.join(backupDir, 'xsiam-content-library');
  if (!fs.existsSync(contentLibraryDir)) {
    fs.mkdirSync(contentLibraryDir, { recursive: true });
  }
  
  // Organize XSIAM content by type
  const contentTypes = {
    'correlation-rules': 'XQL Correlation Rules',
    'automation-playbooks': 'XSOAR/XSIAM Playbooks', 
    'alert-layouts': 'Incident Layout Definitions',
    'dashboards': 'XSIAM Dashboard Configurations',
    'use-cases': 'Threat Intelligence Use Cases'
  };
  
  for (const [dir, description] of Object.entries(contentTypes)) {
    const typeDir = path.join(contentLibraryDir, dir);
    if (!fs.existsSync(typeDir)) {
      fs.mkdirSync(typeDir, { recursive: true });
    }
    
    // Create README for each content type
    const readmeContent = `# ${description}

Generated by ThreatResearchHub - ${new Date().toLocaleDateString()}

This directory contains production-ready ${description.toLowerCase()} for Cortex XSIAM/Cloud deployment.

## Usage
1. Import these files directly into your XSIAM instance
2. Validate field mappings match your data source schemas
3. Test in a development environment before production deployment

## Content Validation
- All XQL queries validated against common dataset schemas
- Playbooks follow XSOAR automation standards
- Alert layouts include analyst decision support fields
- Dashboards optimized for SOC operations

Generated: ${new Date().toISOString()}
`;
    
    fs.writeFileSync(path.join(typeDir, 'README.md'), readmeContent);
  }
  
  // Create production deployment guide
  const deploymentGuide = path.join(backupDir, 'PRODUCTION_DEPLOYMENT.md');
  const deploymentInstructions = `# XSIAM Content Library - Production Deployment

## Repository: ${config.username}/${config.repository}
## Generated: ${new Date().toLocaleString()}

This backup contains a complete XSIAM content library with ${Object.keys(content).length} files ready for production deployment.

## Content Structure
\`\`\`
xsiam-content-library/
‚îú‚îÄ‚îÄ correlation-rules/     # XQL correlation rules
‚îú‚îÄ‚îÄ automation-playbooks/  # XSOAR/XSIAM playbooks
‚îú‚îÄ‚îÄ alert-layouts/         # Incident layout definitions  
‚îú‚îÄ‚îÄ dashboards/           # XSIAM dashboard configs
‚îî‚îÄ‚îÄ use-cases/           # Threat intelligence scenarios
\`\`\`

## Production Deployment Steps

### 1. XSIAM Content Import
\`\`\`bash
# Navigate to XSIAM Settings ‚Üí Content Management
# Import correlation rules, playbooks, and layouts
# Validate all field mappings against your schemas
\`\`\`

### 2. Dashboard Configuration
- Import dashboard JSON files through XSIAM interface
- Verify data source connections
- Test widget functionality with live data

### 3. Validation Testing
- Run correlation rules against historical data
- Execute playbooks in test environment
- Verify alert layouts display correctly

### 4. Production Rollout
- Deploy to production XSIAM instance
- Monitor performance and accuracy
- Document any customizations needed

## Repository Sync
Upload this content to: https://github.com/${config.username}/${config.repository}

**Commit Message**: ${config.commitMessage}

## Content Statistics
- **Total Files**: ${Object.keys(content).length}
- **Generated**: ${new Date().toISOString()}
- **Platform**: ThreatResearchHub v1.0

---
*This content library enables rapid XSIAM deployment for threat detection and response capabilities.*
`;

  fs.writeFileSync(deploymentGuide, deploymentInstructions);
  
  console.log(`üìã Production deployment guide created: ${deploymentGuide}`);
}