XQL Correlations Queries


XDR Data


DataSet Ingest 

Dataset management is the uncompressed numbers and the Ingestion is the compressed numbers. They will always be different and we only measure off the ingestion side

This is for just the NGFW logs:

preset = metrics_view 
|filter _vendor = "PANW"
| comp sum(total_size_bytes) as totalbytes by _log_type
| alter totalGB = divide(totalbytes, 1073741824) 
| sort desc totalGB

You can also configure like this for a higher level view by vendor:

preset = metrics_view 
| comp sum(total_size_bytes) as totalbytes by _vendor
| alter totalGB = divide(totalbytes, 1073741824) 
| sort desc totalGB



Proofpoint

dataset=proofpoint_tap_raw 
|filter to_epoch(clickTime) < to_epoch(threatTime)
|fields sender , senderIP, recipient, url, clickIP , clickTime , messageTime , threatTime , messageID 
|dedup sender, recipient, messageID , url
| join type = inner
    (dataset= panw_ngfw_url_raw  ) as URL  url contains URL.url_domain
| filter is_url_denied = "false"
|dedup sender, recipient, messageID , url


Azure AD - administrator access granted - event hub:

dataset = msft_azure_ad_audit_raw 
| filter (category = "RoleManagement") 
| alter role = targetResources -> []
| alter test = arrayindex(role ,0)
| alter modified = json_extract(test, "$.modifiedProperties")
| alter modified = modified ->[] 
| fields modified | arrayexpand modified | alter newValue = json_extract_scalar(modified , "$.newValue")|  filter newValue in("\"Company Administrator\"", "\"TenantAdmins\"" , "\"Global Administrator\"")


Azure  - ports opened for non-web traffic:

dataset = msft_azure_raw
| filter operationName = "MICROSOFT.NETWORK/NETWORKSECURITYGROUPS/SECURITYRULES/WRITE" and resultType = "Accept"
| alter responseBody = properties  -> responseBody
| alter requestProperties  = json_extract(responseBody,"$.properties")
| alter  sourceAddressPrefix = requestProperties -> sourceAddressPrefix, access = requestProperties -> access, direction = requestProperties -> direction,destinationPortRange = requestProperties -> destinationPortRange , protocol = requestProperties -> protocol,priority = requestProperties -> priority,destinationAddressPrefix = requestProperties -> destinationAddressPrefix, sourcePortRange = requestProperties -> sourcePortRange, claims = json_extract(identity,"$.claims"),role = json_extract_scalar(identity,"$.authorization.evidence.role")
| filter sourceAddressPrefix in ("*",null) and access = "Allow"  and direction = "Inbound" and  destinationPortRange not in ("443","80")
| alter   upn = regextract(claims , "upn\"\:\"(\w*\@.*?)\""),surname=regextract(claims , "surname\"\:\"(\w*)\""),name=regextract(claims , "givenname\"\:\"(\w*)\"")
| fields upn,callerIpAddress,sourceAddressPrefix,sourcePortRange,destinationAddressPrefix, destinationPortRange,protocol,priority,resourceId,correlationId,name,surname,role, access, direction 

Zscaler:

dataset = zscaler_nssweblog_raw 
| filter cs4 in ("Ransomware" , "Virus" , "Trojan")


dataset = zscaler_nssweblog_raw 
| filter cs2 contains "Security"
| filter (act = """Allowed""" and cat = """Phishing""") 

Okta:

dataset = okta_okta_raw  
| filter eventType = "security.threat.detected"

dataset = okta_okta_raw  
| alter threatSuspected = json_extract_scalar(debugContext , "$.debugData.threatSuspected") 
| filter threatSuspected = "true"

config case_sensitive = false 
| dataset = okta_sso_raw
| filter outcome contains "FAILURE"
| fields _time, actor, displayMessage as operation, outcome, client
| alter platform = "Okta", 
	userName = json_extract_scalar(actor, "$.alternateId"),
	userDisplayName = json_extract_scalar(actor, "$.displayName"),
	userId = json_extract_scalar(actor, "$.id"),
	userType = json_extract_scalar(actor, "$.type"),
    auth_outcome = json_extract_scalar(outcome, "$.result"),    
    clientIp = client -> ipAddress,
    userAgent = client -> userAgent.rawUserAgent,
    gl_Country = client -> geographicalContext.country,
    gl_City = client -> geographicalContext.city,
    gl_State = client -> geographicalContext.state 
| comp max(_time) as maxt, count(auth_outcome) as Failures by platform, operation, auth_outcome, clientIp, userName, userType, userId, userDisplayName, userAgent, gl_Country, gl_City, gl_State
| filter Failures > 2

dataset = okta_sso_raw 
| alter Username=json_extract(actor, "$.displayName")
| alter Outcome=json_extract(outcome, "$.result")
| alter User = trim(Username, "\"")
| alter Result = trim(Outcome, "\"")
| filter Result = "FAILURE"
| comp count(_id) as Total by User
| sort desc Total

| filter (User not in ("""bally-*""", """deltatreinc*""", """Okta*""", """peacock-*""")) 
| filter (User not in ("""harmonicint-*""", """turner-*""", """videosoluti-*""", """scheduler-*""", """ctsq-*""", """CableOS Portal""", """vidgoinc-*""")) 
| filter (User not in ("""microsoftco-*""", """*-api""", """cts-*""", """*Scheduler*""", """unknown""", """vosemeapres*""", """beinsportsf-*""", """ctsp-*""")) 
| filter (User not in ("""IT Support""", """cos-sonar*""", """user added*"""))
| filter (User != """Grafana*""")
| view graph type = pie subtype = semi_donut xaxis = User yaxis = Total 

XDR - Event Log:

preset = xdr_event_log 
| filter action_evtlog_event_id = 4649

Office 365 DLP - High

dataset = msft_o365_dlp_raw 
| alter PolicyDetails = PolicyDetails -> []
| alter policy = arrayindex(PolicyDetails ,0)
| alter PolicyName = json_extract_scalar(policy , "$.PolicyName"), Rules = json_extract(policy , "$.Rules")
| alter rules = Rules -> []
| alter rules = arrayindex(rules ,0)
| alter RuleName = json_extract_scalar(rules , "$.RuleName"),Severity = json_extract_scalar(rules , "$.Severity")
| filter Severity  = "High"
| alter SensitiveInformation = json_extract(rules, "$.ConditionsMatched.SensitiveInformation")
| alter sensitive  = SensitiveInformation -> []
| alter sensitive = arrayindex(sensitive ,0)
| alter sensitive = json_extract(sensitive, "$.SensitiveInformationDetections.DetectedValues")

Varnish sev 3 and high alerts:

dataset = varonis_datalert_raw
| filter _raw_log contains "sev=3"

FW Sinkhole Event:

dataset = panw_ngfw_threat_raw 
| filter sub_type = "spyware" and action = "sinkhole"
| join type = left ( dataset = endpoints 
    | fields ip_address as agent_ip_address, user, endpoint_name, group_names
) as endpoints source_ip contains arraystring(endpoints.agent_ip_address, ":")
| filter group_names not contains "Domain Controller"
| comp count_distinct(_id) as num_sinkhole_events by source_ip, user, endpoint_name, file_name, threat_category
| filter num_sinkhole_events >= 15

//

// Hunt for RDP from an affected NGFW in NGFW traffic logs
config case_sensitive = false timeframe = 7d
| dataset = panw_ngfw_traffic_raw 
| filter dest_port = 445
| filter source_ip = "10.4.108.16"
| filter action= "allow" | fields log_time, source_ip, source_port, dest_ip, dest_port


Windows - WEC:

dataset = xdr_data //use the xdr_data dataset which contains the WEC logs
|filter event_type = EVENT_LOG // filter events to show only event_logs
|fields action_evtlog_message as Message, action_evtlog_event_id as EventID // show the event log message and event_id


IP/URI Searches:

Networks_Story:  (this one is not saved) For network data, all relevant logs from the different data sources are stitched to the same network story.

preset = network_story | filter action_local_ip = "x.x.x.x"

Using Dataset: 

Example URL Search dataset:

dataset = panw_ngfw_url_raw
| filter  dest_ip  contains """x.x.x.x""" or dest_ip contains """X.X.X.X"""
|fields dest_port, dest_device_host, dest_ip, source_ip, url_domain 

Saved in the query Library: Example IP Search Dataset NGFW Traffic

dataset = panw_ngfw_traffic_raw
| filter  dest_ip  contains """X.X.X.X""" or dest_ip contains """x.x.x.x"""
|fields dest_port, dest_device_host, dest_ip, source_ip, source_user_info_domain, dest_user_info_domain 


Using DataModels: 

Search NGFW URL Example:

datamodel dataset =  panw_ngfw_url_raw
| filter ((xdm.target.ipv4 contains """X.X.X.X""" or xdm.target.ipv4 contains """X.X.X.X"""))
| fields  xdm.target.url, xdm.target.domain, xdm.target.ipv4, xdm.target.host.hostname, xdm.target.port, xdm.source.port 

Example Multiple IP address:

datamodel dataset =  panw_ngfw_traffic_raw
| filter ((xdm.target.ipv4 contains """X.X.X.X""" or xdm.target.ipv4 contains """X.X.X.X"""))
| fields  xdm.target.url, xdm.target.domain, xdm.target.ipv4, xdm.target.host.hostname, xdm.target.port, xdm.source.port 
*Saved without the | fields

Example Search for URLS:

datamodel dataset =  panw_ngfw_traffic_raw
| filter ((xdm.target.url contains """XXXXX.com""" or xdm.target.url contains """XXXXXX.com"""))

Unit 42 Threat Hunting:
 https://querycrew.docs.pan.run/query-repository/XQL%20Queries/NGFW/NGFW_Network_Operations/NGFW%20Remote%20Access%20Activity/
//Title: NGFW Remote Access Activity
//Description: Intended to be used as an XDR Widget to highlight Remote Access Activity derived from panw_ngfw_traffic logs. 
//Author: Raymond DePalma (Updated by Juwan Rogers)
//Technical QC: Cooper Allen 
//Date: January 24, 2023 
//Dataset: panw_ngfw_traffic_raw
//Requirements: PA NGFW File Logs, PRO enabled
//Tags: NGFW,PANWOpen

config case_sensitive = false timeframe between "begin" and "now"
| dataset = panw_ngfw_traffic_raw
//Filter for remote access applications
| filter app_sub_category = "remote-access"
//Count total events by app, zone, ip and port 
| comp count(_id) as Total by app, from_zone, to_zone, source_ip , dest_ip, dest_port
| sort desc Total

//Use the following sections for IR and CA engagements (turn off default comp/field lines above in order to use these sections)
//Returning relevant fields within a specified timespan (2 different options)
//| fields app, app_category, from_zone, to_zone, session_start_time, source_ip , dest_ip, dest_port, dest_location, session_start_time, bytes_sent, bytes_received, bytes_total, count_of_repeats
//| fields _time, rule_matched, action, app, app_category, app_sub_category, from_zone, to_zone, source_user, source_ip, dest_ip, dest_port, dest_user, users

//Use this section to aggregate data for CA analysis, turn off for reporting
//1st pass
//| comp count(_id) as Total by app, from_zone, to_zone, source_ip , dest_ip, dest_port
//| sort desc Total
//2nd pass
//| filter app in ("[Finding]", "[Finding]")
//| filter source_ip in ("[Finding]", "[Finding]")
//| filter dest_ip in ("[Finding]", "[Finding]")
//| comp count(_id) as Total by app, app_category, from_zone, to_zone, source_ip , dest_ip, dest_port
//| fields app, app_category, from_zone, to_zone, source_ip , dest_ip, dest_port, total
//| sort desc total

//Use this section to put finding in report format for CA, replace [Finding] with what you would like to report (wildcards* accepted), field for filtering can be changed
//| filter app in ("[Finding]", "[Finding]")
//| filter source_ip in ("[Finding]", "[Finding]")
//| filter dest_ip in ("[Finding]", "[Finding]")
//| comp count(_id) as event_count, max(_time) as latest_time, values(dest_location) as dest_locations by source_ip, app, app_category, dest_ip, rule_matched, dest_port
//| alter disc_source = "NGFW - Remote Access"
//| fields source_ip, app, app_category, event_count, dest_ip, dest_locations, rule_matched, latest_time, dest_port, disc_source
//| sort desc latest_time


AWS

Guard Duty

https://docs.google.com/document/d/1efMS--TYjBDMEd-4smxFTA45JBV3U42yxmVTAm1Xass/edit?tab=t.0

[{"rule_id": 17, "name": "AWS - GuardDuty Alerts", "severity": "User Defined", "xql_query": "datamodel dataset = aws_guardduty_raw \r\n| filter xdm.alert.name != null\r\n| fields *", "is_enabled": true, "description": "AWS - GuardDuty Alerts", "alert_name": "$xdm.alert.name", "alert_category": "User Defined", "alert_type": null, "alert_description": "$xdm.alert.description", "alert_domain": "DOMAIN_SECURITY", "alert_fields": {}, "execution_mode": "REAL_TIME", "search_window": null, "simple_schedule": null, "timezone": null, "crontab": null, "suppression_enabled": false, "suppression_duration": null, "suppression_fields": null, "dataset": "alerts", "user_defined_severity": "xdm.alert.severity", "user_defined_category": "xdm.alert.subcategory", "mitre_defs": {}, "investigation_query_link": "", "drilldown_query_timeframe": "ALERT", "mapping_strategy": "AUTO", "action": "ALERTS", "lookup_mapping": []}]


AWS - From Yellow:

https://docs.google.com/spreadsheets/d/1ChB42hCpbDEAWE9mJyBGu4yZw9b5YkahMGPJNjT_B2M/edit?gid=0#gid=0


Lookback queries Office635, Active Directory, Fortigate:

Not sure if you guys have played with grok yet - unfortunately no api connectivity yet, but it seems to be able to pull off xql queries better than all of the others:

all I gave it was a scrolling screen shot of the schema so it knows what fields it can use and a basic use case definition and it did this with comments and stuff - nice little lookback type query (edited) 


dataset = msft_o365_azure_ad_raw  // Defines the data source as Office 365 Azure AD raw logs.
| filter ErrorNumber != "0" or ResultStatus = "Success"  // Filters for login failures (ErrorNumber not equal to "0" as a string) or successful logins (ResultStatus = "Success"), addressing the string expectation.
| fields UserId, ClientIP, _time, ErrorNumber, ResultStatus  // Selects key fields, using ErrorNumber as the failure indicator.
| bin _time span = 30m  // Groups events into 30-minute time bins for the 30-minute window constraint.
| comp count(if(ErrorNumber != "0", 1, null)) as failure_count, count(if(ResultStatus = "Success", 1, null)) as success_count by UserId, ClientIP  // Counts failures (ErrorNumber not "0") and successes per UserId and ClientIP within each bin.
| filter failure_count >= 15 and success_count > 0  // Filters for groups with 15 or more failures and at least one success, meeting the threshold criteria.
| limit 1000  // Limits the result set to 1000 records for performance.


Manually built query to do the same thing - but of course grok doesnt know about normalized datasets like "cloud_audit_logs"

dataset = cloud_audit_logs
|filter operation_name = ENUM.Login
|fields identity_name , identity_uuid , caller_ip , identity_type , operation_status, user_agent , referenced_resource , referenced_resource_name , operation_name_orig
|alter auth_attempt = if(operation_status = ENUM.Success, "success", "failure")
|windowcomp lag(auth_attempt) by identity_name, identity_uuid sort asc _time as prevAuthAttemptResult
|windowcomp count(auth_attempt) by identity_name, identity_uuid, prevAuthAttemptResult as totalAuthAttempts
|filter totalAuthAttempts >= 10 and auth_attempt = "success" and prevAuthAttemptResult = "failure"

dataset = fortinet_fortigate_raw  // Defines the data source as Fortinet FortiGate raw logs.
| filter msg contains "failed" and msg contains "vpn"  // Filters for events where msg indicates a failure and contains "vpn" to target VPN login attempts.
| fields src, duser, _time  // Selects source IP (src), destination user (duser), and timestamp (_time) for analysis.
| bin _time span = 10m  // Groups events into 10-minute time bins for the 10-minute window constraint.
| comp count() as attempt_count, count_distinct(duser) as unique_users by src, _time  // Counts total failed attempts and unique destination users per source IP within each 10-minute bin.
| filter unique_users >= 4  // Filters for IPs targeting 4 or more unique users, meeting the use case threshold.
| limit 1000  // Limits the result set to 1000 records for performance.

dataset = microsoft_windows_raw  // Defines the data source as Microsoft Windows raw logs containing event data.
| filter event_id in (4720, 4722, 4723, 4724, 4725, 4738)  // Filters for event IDs associated with account changes (e.g., user created, enabled, deleted, etc.).
| alter changer = json_extract_scalar(event_data, "$.SubjectUserName"), user = json_extract_scalar(event_data, "$.TargetUserName")  // Extracts the changer (SubjectUserName) and target user (TargetUserName) from the event_data JSON field.
| fields changer, user, _time  // Selects the extracted changer and target fields, along with _time.
| bin _time span = 1h  // Groups events into 1-hour bins based on _time, enforcing the 1-hour window.
| comp count_distinct(user) as unique_targets, count() as change_count by changer, _time  // Uses count_distinct(target) to count unique TargetUserName values per changer within each 1-hour bin, and count() for total changes, aligning with the provided syntax.
| filter unique_targets > 5  // Filters to include only changers who modified more than 5 unique targets, meeting the use case threshold.
| limit 1000  // Limits the result set to 1000 records for performance.


